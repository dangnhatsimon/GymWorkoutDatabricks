{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f60a55c8-577b-4488-8069-2527ffa68e6a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a707d6c3-b286-4454-8a79-260d952c78b4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Scheduling a Batch Job\n",
    "\n",
    "This notebook is designed to be scheduled as a batch job. \n",
    "\n",
    "As a general rule, before scheduling notebooks, make sure you comment out:\n",
    "- Any file removal commands added during development\n",
    "- Any commands dropping or creating databases or tables (unless you wish these to be created fresh with each execution)\n",
    "- Any arbitrary actions/SQL queries that materialize results to the notebook (unless a human will regularly review this visual output)\n",
    "\n",
    "### Scheduling Against an Interactive Cluster\n",
    "\n",
    "Because our data is small and the query we run here will complete fairly quickly, we'll take advantage of our already-on compute while scheduling this notebook.\n",
    "\n",
    "You may choose to manually trigger this job, or set it to a schedule to update each minute.\n",
    "\n",
    "You can click **`Run Now`** if desired, or just wait until the top of the next minute for this to trigger automatically.\n",
    "\n",
    "### Best Practice: Warm Pools\n",
    "\n",
    "During this demo, we're making the conscious choice to take advantage of already-on compute to reduce friction and complexity for getting our code running. In production, jobs like this one (short duration and triggered frequently) should be scheduled against <a href=\"https://docs.microsoft.com/en-us/azure/databricks/clusters/instance-pools/\" target=\"_blank\">warm pools</a>.\n",
    "\n",
    "Pools provide you the flexibility of having compute resources ready for scheduling jobs against while removing DBU charges for idle compute. DBUs billed are for jobs rather than all-purpose workloads (which is a lower cost). Additionally, using pools instead of interactive clusters eliminates the potential for resource contention between jobs sharing a single cluster or between scheduled jobs and interactive queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "530cfc9f-09fc-472f-a8d4-372478240e7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../../Includes/Classroom-Setup-8.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1521fd31-72e8-49ae-8005-0d72593eef7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# user_bins\n",
    "def age_bins(dob_col):\n",
    "    age_col = F.floor(F.months_between(F.current_date(), dob_col)/12).alias(\"age\")\n",
    "    return (F.when((age_col < 18), \"under 18\")\n",
    "            .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "            .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "            .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "            .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "            .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "            .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "            .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "            .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "            .when((age_col >= 95), \"95+\")\n",
    "            .otherwise(\"invalid age\").alias(\"age\"))\n",
    "\n",
    "lookupDF = spark.table(\"user_lookup\").select(\"alt_id\", \"user_id\")\n",
    "binsDF = spark.table(\"users\").join(lookupDF, [\"alt_id\"], \"left\").select(\"user_id\", age_bins(F.col(\"dob\")),\"gender\", \"city\", \"state\")\n",
    "\n",
    "(binsDF.write\n",
    "       .format(\"delta\")\n",
    "       .mode(\"overwrite\")\n",
    "       .saveAsTable(\"user_bins\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89407595-931e-440e-9d99-b3ace52bedf8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# completed_workouts\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW TEMP_completed_workouts AS (\n",
    "      SELECT a.user_id, a.workout_id, a.session_id, a.start_time start_time, b.end_time end_time, a.in_progress AND (b.in_progress IS NULL) in_progress\n",
    "      FROM (\n",
    "        SELECT user_id, workout_id, session_id, time start_time, null end_time, true in_progress\n",
    "        FROM workouts_silver\n",
    "        WHERE action = \"start\") a\n",
    "      LEFT JOIN (\n",
    "        SELECT user_id, workout_id, session_id, null start_time, time end_time, false in_progress\n",
    "        FROM workouts_silver\n",
    "        WHERE action = \"stop\") b\n",
    "      ON a.user_id = b.user_id AND a.session_id = b.session_id\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "(spark.table(\"TEMP_completed_workouts\").write\n",
    "      .mode(\"overwrite\")\n",
    "      .saveAsTable(\"completed_workouts\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0133146b-5c74-4266-ae8f-fa9191bad8a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#workout_bpm\n",
    "spark.readStream.table(\"heart_rate_silver\").createOrReplaceTempView(\"TEMP_heart_rate_silver\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "  SELECT d.user_id, d.workout_id, d.session_id, time, heartrate\n",
    "  FROM TEMP_heart_rate_silver c\n",
    "  INNER JOIN (\n",
    "    SELECT a.user_id, b.device_id, workout_id, session_id, start_time, end_time\n",
    "    FROM completed_workouts a\n",
    "    INNER JOIN user_lookup b\n",
    "    ON a.user_id = b.user_id) d\n",
    "  ON c.device_id = d.device_id AND time BETWEEN start_time AND end_time\n",
    "  WHERE c.bpm_check = 'OK'\"\"\").createOrReplaceTempView(\"TEMP_workout_bpm\")\n",
    "\n",
    "query = (spark.table(\"TEMP_workout_bpm\")\n",
    "              .writeStream\n",
    "              .outputMode(\"append\")\n",
    "              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/workout_bpm\")\n",
    "              .trigger(availableNow=True)\n",
    "              .table(\"workout_bpm\"))\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f2cb13e-4cc6-4e0a-9063-965a93167924",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# workout_bpm_summary\n",
    "spark.readStream.table(\"workout_bpm\").createOrReplaceTempView(\"TEMP_workout_bpm\")\n",
    "\n",
    "df = (spark.sql(\"\"\"\n",
    "SELECT workout_id, session_id, a.user_id, age, gender, city, state, min_bpm, avg_bpm, max_bpm, num_recordings\n",
    "FROM user_bins a\n",
    "INNER JOIN\n",
    "  (SELECT user_id, workout_id, session_id, MIN(heartrate) min_bpm, MEAN(heartrate) avg_bpm, MAX(heartrate) max_bpm, COUNT(heartrate) num_recordings\n",
    "  FROM TEMP_workout_bpm\n",
    "  GROUP BY user_id, workout_id, session_id) b\n",
    "ON a.user_id = b.user_id\"\"\"))\n",
    "\n",
    "query = (df.writeStream\n",
    "           .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/workout_bpm_summary\")\n",
    "           .outputMode(\"complete\")\n",
    "           .trigger(availableNow=True)\n",
    "           .table(\"workout_bpm_summary\"))\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9415274e-a8b1-43c1-987f-48efc91dff58",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Unlike other lessons, we will **NOT** be be executing our **`DA.cleanup()`** command<br/>\n",
    "as we want these assets to persist through all the notebooks in this demo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0e7ba1e5-ffc3-4c31-9cf6-3212db2587f7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "3 - Schedule Batch Jobs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}