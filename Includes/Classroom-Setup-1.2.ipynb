{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5eabef8-c361-4777-ab2e-43bbd2a7005f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_databricks-academy-helper $lesson=\"1.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0818372a-a558-41cc-9c4e-74f75ae59834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_utility-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bbf6efb-8026-4cee-8efb-ade78924023f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_raw_data():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(f\"Creating raw_data\", end=\"...\")\n",
    "    \n",
    "    (spark.read\n",
    "          .load(f\"{DA.hidden.datasets}/bronze\")\n",
    "          .select(F.col(\"key\").cast(\"string\").alias(\"key\"), \"value\", \"topic\", \"partition\", \"offset\", \"timestamp\")\n",
    "          .filter(\"week_part > '2019-49'\")\n",
    "          .write\n",
    "          .format(\"parquet\")\n",
    "          .option(\"path\", f\"{DA.paths.working_dir}/raw_parquet\")\n",
    "          .saveAsTable(\"raw_data\"))\n",
    "\n",
    "    # DA.raw_data_tbl = \"raw_data\" \n",
    "    # spark.conf.set(\"da.raw_data_tbl\", DA.raw_data_tbl)\n",
    "    # spark.read.format(\"parquet\").load(f\"{DA.paths.working_dir}/raw_parquet\").createOrReplaceTempView(\"raw_data\")\n",
    "\n",
    "    total = spark.read.table(\"raw_data\").count()\n",
    "    assert total == 6443331, f\"Expected 6,443,331 records, found {total:,} in raw_data\"    \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fbe41c22-6834-4e17-ae55-5e944657db06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()\n",
    "DA.init()\n",
    "\n",
    "create_raw_data()\n",
    "\n",
    "DA.conclude_setup()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Classroom-Setup-1.2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}