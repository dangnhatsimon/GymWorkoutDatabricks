{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4e1f524b-cc05-451e-985c-5c48ae35c451",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Paths():\n",
    "    def __init__(self, working_dir, clean_lesson):\n",
    "        self.working_dir = working_dir\n",
    "        self.user_db = f\"{working_dir}/{clean_lesson}.db\"\n",
    "        self.checkpoints = f\"{working_dir}/_checkpoints\"    \n",
    "            \n",
    "    def exists(self, path):\n",
    "        try: return len(dbutils.fs.ls(path)) >= 0\n",
    "        except Exception:return False\n",
    "\n",
    "    def print(self, padding=\"  \"):\n",
    "        max_key_len = 0\n",
    "        for key in self.__dict__: max_key_len = len(key) if len(key) > max_key_len else max_key_len\n",
    "        for key in self.__dict__:\n",
    "            label = f\"{padding}DA.paths.{key}:\"\n",
    "            print(label.ljust(max_key_len+13) + DA.paths.__dict__[key])\n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__dict__.__repr__().replace(\", \", \",\\n\").replace(\"{\",\"\").replace(\"}\",\"\").replace(\"'\",\"\")\n",
    "    \n",
    "class DBAcademyHelper():\n",
    "    def __init__(self, lesson):\n",
    "        import re, time\n",
    "        \n",
    "        assert lesson is not None, f\"The lesson must be specified\"\n",
    "\n",
    "        self.initialized = False\n",
    "        self.start = int(time.time())\n",
    "        \n",
    "        self.course_name = \"adewd\"          # Name should be the full name, not the code.\n",
    "        self.course_code = self.course_name # Hacking this as a temporary solution\n",
    "        \n",
    "        self.lesson = lesson.lower()\n",
    "\n",
    "        # Define username\n",
    "        self.username = spark.sql(\"SELECT current_user()\").first()[0]\n",
    "        clean_username = re.sub(\"[^a-zA-Z0-9]\", \"_\", self.username)\n",
    "\n",
    "        self.db_name_prefix = f\"dbacademy_{clean_username}_{self.course_name}\"\n",
    "        # self.source_db_name = None\n",
    "\n",
    "        self.working_dir_prefix = f\"dbfs:/user/{self.username}/dbacademy/{self.course_name}\"\n",
    "        \n",
    "        clean_lesson = re.sub(\"[^a-zA-Z0-9]\", \"_\", self.lesson)\n",
    "        working_dir = f\"{self.working_dir_prefix}/{self.lesson}\"\n",
    "        self.paths = Paths(working_dir, clean_lesson)\n",
    "        self.hidden = Paths(working_dir, clean_lesson)\n",
    "        self.db_name = f\"{self.db_name_prefix}_{clean_lesson}\"\n",
    "\n",
    "        self.hidden.datasets = f\"{self.working_dir_prefix}/datasets\"\n",
    "            \n",
    "    def init(self, create_db=True):\n",
    "        spark.catalog.clearCache()\n",
    "        self.create_db = create_db\n",
    "        \n",
    "        if create_db:\n",
    "            print(f\"\\nCreating the database \\\"{self.db_name}\\\"\")\n",
    "            spark.sql(f\"CREATE DATABASE IF NOT EXISTS {self.db_name} LOCATION '{self.paths.user_db}'\")\n",
    "            spark.sql(f\"USE {self.db_name}\")\n",
    "            \n",
    "        self.initialized = True\n",
    "\n",
    "    def cleanup(self):\n",
    "        for stream in spark.streams.active:\n",
    "            print(f\"Stopping the stream \\\"{stream.name}\\\"\")\n",
    "            try: stream.stop()\n",
    "            except: pass # Bury any exceptions\n",
    "            try: stream.awaitTermination()\n",
    "            except: pass # Bury any exceptions\n",
    "        \n",
    "        if spark.sql(f\"SHOW DATABASES\").filter(f\"databaseName == '{self.db_name}'\").count() == 1:\n",
    "            print(f\"Dropping the database \\\"{self.db_name}\\\"\")\n",
    "            spark.sql(f\"DROP DATABASE {self.db_name} CASCADE\")\n",
    "        \n",
    "        if self.paths.exists(self.paths.working_dir):\n",
    "            print(f\"Removing the working directory \\\"{self.paths.working_dir}\\\"\")\n",
    "            dbutils.fs.rm(self.paths.working_dir, True)\n",
    "        \n",
    "        # FIXME: Commented out because it might break during parallel testing.\n",
    "        # self.databricks_api('POST', '2.0/secrets/scopes/delete', on_error=\"ignore\", scope=\"DA-ADE3.03\")\n",
    "        \n",
    "        # Make sure that they were not modified\n",
    "        if self.initialized: validate_datasets()\n",
    "\n",
    "    def conclude_setup(self, validate=True):\n",
    "        import time\n",
    "        \n",
    "        spark.conf.set(\"da.db_name\", self.db_name)\n",
    "        for key in self.paths.__dict__:\n",
    "            spark.conf.set(f\"da.paths.{key.lower()}\", self.paths.__dict__[key])\n",
    "        \n",
    "        print(\"\\nPredefined Paths:\")\n",
    "        DA.paths.print()\n",
    "\n",
    "        if self.create_db:\n",
    "            print(f\"\\nPredefined tables in {self.db_name}:\")\n",
    "            tables = spark.sql(f\"SHOW TABLES IN {self.db_name}\").filter(\"isTemporary == false\").select(\"tableName\").collect()\n",
    "            if len(tables) == 0: print(\"  -none-\")\n",
    "            for row in tables: print(f\"  {row[0]}\")\n",
    "                \n",
    "        print()\n",
    "        if validate: validate_datasets()\n",
    "        print(f\"\\nSetup completed in {int(time.time())-self.start} seconds\")\n",
    "\n",
    "    @staticmethod\n",
    "    def databricks_api(http_method, path, *, on_error=\"raise\", **data):\n",
    "        \"\"\"\n",
    "        Invoke the Databricks REST API for the current workspace as the current user.\n",
    "        \n",
    "        Args:\n",
    "            http_method: 'GET', 'PUT', 'POST', or 'DELETE'\n",
    "            path: The path to append to the URL for the API endpoint, excluding the leading '/'.\n",
    "                For example: path=\"2.0/secrets/put\"\n",
    "            on_error: 'raise', 'ignore', or 'return'.\n",
    "                'raise'  means propogate the HTTPError (Default)\n",
    "                'ignore' means return None\n",
    "                'return' means return the error message as parsed json if possible, otherwise as text.\n",
    "\n",
    "        Returns:\n",
    "            The return value of the API call as parsed JSON.  If the result is invalid JSON then the\n",
    "            result will be returned as plain text.\n",
    "\n",
    "        Raises:\n",
    "            requests.HTTPError: If the API returns an error and on_error='raise'.\n",
    "        \"\"\"\n",
    "        import requests, json\n",
    "        url = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiUrl().getOrElse(None)+\"/api/\"\n",
    "        token = dbutils.notebook.entry_point.getDbutils().notebook().getContext().apiToken().getOrElse(None)\n",
    "        web = requests.Session()\n",
    "        web.headers = {'Authorization': 'Bearer ' + token, 'Content-Type': 'text/json'}\n",
    "        if http_method == 'GET':\n",
    "            params = {k: str(v).lower() if isinstance(value, bool) else v for k,v in data.items()}\n",
    "            resp = web.request(http_method, url + path, params = params)\n",
    "        else:\n",
    "            resp = web.request(http_method, url + path, data = json.dumps(data))\n",
    "        if on_error.lower() in [\"raise\", \"throw\", \"error\"]:\n",
    "            resp.raise_for_status()\n",
    "        elif on_error.lower() in [\"ignore\", \"none\"] and not (200 <= resp.status_code < 300):\n",
    "            return None\n",
    "        elif on_error.lower() not in [\"return\", \"return_error\"]:\n",
    "            raise Exception(\"on_error argument must be one of 'raise', 'ignore' or 'return'\")\n",
    "        try:\n",
    "            return resp.json()\n",
    "        except json.JSONDecodeError as e:\n",
    "            return resp.body\n",
    "\n",
    "    def get_username_hash(self):\n",
    "        \"\"\"\n",
    "        Utility method to split the user's email address, dropping the domain, and then creating a hash based on the full email address and course_code. The primary usage of this function is in creating the user's database, but is also used in creating SQL Endpoints, DLT Piplines, etc - any place we need a short, student-specific name.\n",
    "        \"\"\"\n",
    "        da_name = self.username.split(\"@\")[0]                                   # Split the username, dropping the domain\n",
    "        da_hash = abs(hash(f\"{self.username}-{self.course_code}\")) % 10000      # Create a has from the full username and course code\n",
    "        return da_name, da_hash\n",
    "        \n",
    "          \n",
    "dbutils.widgets.text(\"lesson\", \"None\")\n",
    "lesson = dbutils.widgets.get(\"lesson\")\n",
    "DA = DBAcademyHelper(None if lesson == \"None\" else lesson)\n",
    "\n",
    "# Address the larger stability of this course by setting this value globally.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", sc.defaultParallelism)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "_databricks-academy-helper",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}