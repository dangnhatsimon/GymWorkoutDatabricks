{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e48f365-fd07-4e26-8702-e4a27cc27b62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "This script provides a custom Streaming Query Listener that log progress as JSON files. \n",
    "\n",
    "For more information see <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#reporting-metrics-programmatically-using-asynchronous-apis\" target=\"_blank\">the Structured Streaming Programming Guide, Reporting Metrics Programmatically-using Asynchronous APIs</a>\n",
    "\n",
    "To reset the target logging directory, change the calling cell to **`%run ../Includes/StreamingQueryListener $reset=\"true\"`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc7c879b-8aee-41a0-a9f5-9a9ea5457186",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.widgets.text(\"reset\", \"false\")\n",
    "val reset = dbutils.widgets.get(\"reset\")\n",
    "\n",
    "if(reset==\"true\") {\n",
    "  dbutils.fs.rm(spark.conf.get(\"da.paths.streaming_logs_json\"), true)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8f6f724-a769-4fdc-89d8-b707decc5928",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import org.apache.commons.io.FileUtils\n",
    "import org.apache.spark.sql.streaming.StreamingQueryListener._\n",
    "import org.apache.spark.sql.streaming._\n",
    "\n",
    "import java.io.File\n",
    "\n",
    "class CustomListener() extends StreamingQueryListener {\n",
    "\n",
    "  // Sink\n",
    "  private val fileDirectory = spark.conf.get(\"da.paths.streaming_logs_json\").replaceAll(\"dbfs:/\", \"/dbfs/\")\n",
    "\n",
    "  // Modify StreamingQueryListener Methods \n",
    "  override def onQueryStarted(event: QueryStartedEvent): Unit = {\n",
    "  }\n",
    "\n",
    "  override def onQueryTerminated(event: QueryTerminatedEvent): Unit = {\n",
    "  }\n",
    "\n",
    "  // Send Query Progress metrics to DBFS \n",
    "  override def onQueryProgress(event: QueryProgressEvent): Unit = {\n",
    "    try {\n",
    "      \n",
    "      val file = new File(s\"${fileDirectory}/${event.progress.name}_${event.progress.id}_${event.progress.batchId}.json\")\n",
    "      println(s\"Writing $file\")\n",
    "      \n",
    "      val result_touch = FileUtils.touch(file)\n",
    "      println(s\"  Touch: $result_touch\")\n",
    "      \n",
    "      val result_write = FileUtils.writeStringToFile(file, event.progress.json)\n",
    "      println(s\"  Write: $result_write\")\n",
    "      println(\"-\"*80)\n",
    "      \n",
    "    } catch {\n",
    "      case e: Exception => println(s\"Failed to print\\n$e\")\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "dbutils.fs.mkdirs(spark.conf.get(\"da.paths.streaming_logs_json\"))\n",
    "\n",
    "val streamingListener = new CustomListener()\n",
    "spark.streams.addListener(streamingListener)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "scala",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "StreamingQueryListener",
   "widgets": {}
  },
  "language_info": {
   "name": "scala"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}