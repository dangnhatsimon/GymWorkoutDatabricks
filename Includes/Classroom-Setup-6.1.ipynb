{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "09e4a26b-8fea-4218-9173-895bc90723a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_databricks-academy-helper $lesson=\"6.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68be2e31-bb46-485b-9a13-43baf641b210",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./_utility-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5250fded-4914-45a4-811b-a12f17d4e259",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.paths.raw_user_reg = f\"{DA.paths.user_db}/pii/raw_user_reg\"\n",
    "\n",
    "class UserRegStreamingFactory:\n",
    "    def __init__(self, starting_batch=0, max_batch=15):\n",
    "        self.batch = starting_batch\n",
    "        self.max_batch = max_batch\n",
    "\n",
    "    def load_batch(self, batches):\n",
    "        from pyspark.sql import functions as F\n",
    "        \n",
    "        df = (spark.read\n",
    "              .format(\"json\")\n",
    "              .schema(\"device_id long, mac_address string, registration_timestamp double, user_id long\")\n",
    "              .load(f\"{DA.hidden.datasets}/user-reg\")\n",
    "              .withColumn(\"date\", F.col(\"registration_timestamp\").cast(\"timestamp\").cast(\"date\"))\n",
    "              .withColumn(\"batch\", F.when(F.col(\"date\") < \"2019-12-01\", F.lit(0)).otherwise(F.dayofmonth(F.col(\"date\"))))\n",
    "              .drop(\"date\")\n",
    "              .filter(F.col(\"batch\").isin(batches))\n",
    "              .drop(\"batch\")\n",
    "              .cache())\n",
    "\n",
    "        df.write.mode(\"append\").format(\"json\").save(DA.paths.raw_user_reg)\n",
    "        return df.count()\n",
    "        \n",
    "    def load(self, continuous=False):\n",
    "        import time\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        start = int(time.time())\n",
    "\n",
    "        if self.batch > self.max_batch:\n",
    "            print(\"Data source exhausted\\n\")\n",
    "            \n",
    "        elif not continuous:\n",
    "            print(f\"Loading batch #{self.batch} to raw_user_reg\", end=\"...\")\n",
    "            total = self.load_batch([self.batch])\n",
    "            self.batch += 1\n",
    "            \n",
    "        else:\n",
    "            print(\"Loading all batches to raw_user_reg\", end=\"...\")\n",
    "            batches = list(range(self.batch, self.max_batch+1))\n",
    "            total = self.load_batch(batches)\n",
    "            self.batch = self.max_batch+1\n",
    "            \n",
    "        print(f\"({int(time.time())-start} seconds, {total:,} records)\")\n",
    "\n",
    "        spark.sql(\"DROP TABLE IF EXISTS final_df\")\n",
    "            \n",
    "DA.user_reg_stream = UserRegStreamingFactory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17e712c3-8f04-44b0-a77d-b5beecd6e5cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()\n",
    "DA.init()\n",
    "\n",
    "DA.create_bronze_table()\n",
    "print()\n",
    "\n",
    "DA.user_reg_stream.load()\n",
    "\n",
    "# Stores salt=\"beans\" in the secrets store, if permitted.  Exceptions are suppressed.\n",
    "DA.databricks_api('POST', '2.0/secrets/scopes/create', on_error='return', scope=\"DA-ADE3.03\", initial_manage_principal=\"users\")\n",
    "DA.databricks_api('POST', '2.0/secrets/put',           on_error='return', scope=\"DA-ADE3.03\", key=\"salt\", string_value=\"BEANS\")\n",
    "\n",
    "DA.conclude_setup()\n",
    "\n",
    "None\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Classroom-Setup-6.1",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}