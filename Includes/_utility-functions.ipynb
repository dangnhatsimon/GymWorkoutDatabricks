{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7a411b93-052c-46a0-a4cd-68c13788bbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _install_datasets(reinstall=False):\n",
    "    import time\n",
    "\n",
    "    min_time = \"3 min\"\n",
    "    max_time = \"10 min\"\n",
    "\n",
    "    data_source_uri = \"wasbs://courseware@dbacademy.blob.core.windows.net/advanced-data-engineering-with-databricks/v01\"\n",
    "    print(f\"The source for this dataset is\\n{data_source_uri}/\\n\")\n",
    "\n",
    "    print(f\"Your dataset directory is\\n{DA.hidden.datasets}\\n\")\n",
    "    existing = DA.paths.exists(DA.hidden.datasets)\n",
    "\n",
    "    if not reinstall and existing:\n",
    "        print(f\"Skipping install of existing dataset.\")\n",
    "        print()\n",
    "        validate_datasets()\n",
    "        return \n",
    "\n",
    "    # Remove old versions of the previously installed datasets\n",
    "    if existing:\n",
    "        print(f\"Removing previously installed datasets from\\n{DA.hidden.datasets}\")\n",
    "        dbutils.fs.rm(DA.hidden.datasets, True)\n",
    "\n",
    "    print(f\"\"\"Installing the datasets to {DA.hidden.datasets}\"\"\")\n",
    "\n",
    "    print(f\"\"\"\\nNOTE: The datasets that we are installing are located in Washington, USA - depending on the\n",
    "          region that your workspace is in, this operation can take as little as {min_time} and \n",
    "          upwards to {max_time}, but this is a one-time operation.\"\"\")\n",
    "\n",
    "    files = dbutils.fs.ls(data_source_uri)\n",
    "    print(f\"\\nInstalling {len(files)+2} datasets: \")\n",
    "    \n",
    "    install_start = int(time.time())\n",
    "    for f in files:\n",
    "        start = int(time.time())\n",
    "        print(f\"Copying /{f.name[:-1]}\", end=\"...\")\n",
    "\n",
    "        dbutils.fs.cp(f\"{data_source_uri}/{f.name}\", f\"{DA.hidden.datasets}/{f.name}\", True)\n",
    "        print(f\"({int(time.time())-start} seconds)\")\n",
    "\n",
    "    # TODO - move this into the storage so that we don't have to do it here\n",
    "    start = int(time.time())\n",
    "    print(f\"Copying /user-lookup\", end=\"...\")\n",
    "    (spark.read\n",
    "          .format(\"json\")\n",
    "          .schema(\"device_id long, mac_address string, registration_timestamp double, user_id long\")\n",
    "          .load(f\"{DA.hidden.datasets}/user-reg\")\n",
    "          .selectExpr(f\"sha2(concat(user_id,'BEANS'), 256) AS alt_id\", \"device_id\", \"mac_address\", \"user_id\")\n",
    "          .coalesce(1)\n",
    "          .write\n",
    "          .save(f\"{DA.hidden.datasets}/user-lookup\"))\n",
    "    print(f\"({int(time.time())-start} seconds)\")\n",
    "\n",
    "    # TODO - move this into the storage so that we don't have to do it here\n",
    "    start = int(time.time())\n",
    "    print(f\"Copying /gym-mac-logs\", end=\"...\")\n",
    "    (spark.read\n",
    "          .schema(\"first_timestamp double, gym long, last_timestamp double, mac string\")\n",
    "          .json(f\"{DA.hidden.datasets}/gym-logs\")\n",
    "          .coalesce(1)\n",
    "          .write\n",
    "          .save(f\"{DA.hidden.datasets}/gym-mac-logs\"))\n",
    "    print(f\"({int(time.time())-start} seconds)\")\n",
    "    \n",
    "    print()\n",
    "    validate_datasets()\n",
    "    print(f\"\"\"\\nThe install of the datasets completed successfully in {int(time.time())-install_start} seconds.\"\"\")  \n",
    "\n",
    "DA.install_datasets = _install_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03b592c8-8f5d-4d00-b511-ba08d639db88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def validate_path(expected, path):\n",
    "    files = dbutils.fs.ls(path)\n",
    "    message = f\"Expected {expected} files, found {len(files)} in {path}\"\n",
    "    for file in files:\n",
    "        message += f\"\\n{file.path}\"\n",
    "    \n",
    "    if len(files) != expected:\n",
    "      display(files)\n",
    "      raise AssertionError(message)\n",
    "\n",
    "def validate_datasets():  \n",
    "    import time\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(\"Validating datasets\", end=\"...\")\n",
    "\n",
    "    validate_path(8, f\"{DA.hidden.datasets}\")\n",
    "    validate_path(4, f\"{DA.hidden.datasets}/bronze\")\n",
    "    validate_path(4, f\"{DA.hidden.datasets}/bronze/topic=bpm\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=bpm/week_part=2019-48\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=bpm/week_part=2019-49\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=bpm/week_part=2019-50\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=bpm/week_part=2019-51\")\n",
    "\n",
    "    validate_path(20, f\"{DA.hidden.datasets}/bronze/topic=user_info\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-16\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-19\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-20\")\n",
    "    validate_path(3, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-21\")\n",
    "    validate_path(3, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-22\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-23\")\n",
    "    validate_path(5, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-24\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-26\")\n",
    "    validate_path(5, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-27\")\n",
    "    validate_path(6, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-28\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-29\")\n",
    "    validate_path(3, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-30\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-31\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-32\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-33\")\n",
    "    validate_path(1, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-34\")\n",
    "    validate_path(5, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-48\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-49\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-50\")\n",
    "    validate_path(3, f\"{DA.hidden.datasets}/bronze/topic=user_info/week_part=2019-51\")\n",
    "\n",
    "    validate_path(4, f\"{DA.hidden.datasets}/bronze/topic=workout\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=workout/week_part=2019-48\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=workout/week_part=2019-49\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=workout/week_part=2019-50\")\n",
    "    validate_path(10, f\"{DA.hidden.datasets}/bronze/topic=workout/week_part=2019-51\")\n",
    "\n",
    "    validate_path(9, f\"{DA.hidden.datasets}/date-lookup\")\n",
    "    validate_path(5, f\"{DA.hidden.datasets}/date-lookup/_delta_log\")\n",
    "\n",
    "    validate_path(85, f\"{DA.hidden.datasets}/gym-logs\")\n",
    "\n",
    "    validate_path(11, f\"{DA.hidden.datasets}/kafka-30min\")\n",
    "    validate_path(7, f\"{DA.hidden.datasets}/kafka-30min/_delta_log\")\n",
    "\n",
    "    validate_path(2, f\"{DA.hidden.datasets}/pii\")\n",
    "    validate_path(13, f\"{DA.hidden.datasets}/pii/raw\")\n",
    "    validate_path(9, f\"{DA.hidden.datasets}/pii/raw/_delta_log\")\n",
    "    validate_path(2, f\"{DA.hidden.datasets}/pii/silver\")\n",
    "    validate_path(5, f\"{DA.hidden.datasets}/pii/silver/_delta_log\")\n",
    "\n",
    "    validate_path(100, f\"{DA.hidden.datasets}/user-reg\")\n",
    "\n",
    "    validate_path(2, f\"{DA.hidden.datasets}/user-lookup\")\n",
    "    # validate_path(0, f\"{DA.hidden.datasets}/user-lookup/_delta_log\")\n",
    "\n",
    "    validate_path(2, f\"{DA.hidden.datasets}/gym-mac-logs\")\n",
    "    # validate_path(0, f\"{DA.hidden.datasets}/gym-mac-logs/_delta_log\")\n",
    "\n",
    "    print(f\"({int(time.time())-start} seconds)\")   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab3f186d-b91a-4ed7-9dcd-2e0c4814873d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _block_until_stream_is_ready(query=None, name=None, min_batches=2):\n",
    "    import time\n",
    "\n",
    "    for q in spark.streams.active:\n",
    "        query = q if q.name == name and name is not None else query\n",
    "\n",
    "    while len(query.recentProgress) < min_batches:\n",
    "        time.sleep(5) # Give it a couple of seconds\n",
    "\n",
    "    print(f\"The stream has processed {len(query.recentProgress)} batchs\")\n",
    "\n",
    "DA.block_until_stream_is_ready = _block_until_stream_is_ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28a828c6-2ab8-425b-93e8-70d00523d064",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class TaskConfig():\n",
    "    def __init__(self, name, resource_type, resource, pipeline_id=None, depends_on=[], cluster=\"shared_cluster\", params={}):\n",
    "        self.name = name\n",
    "        self.resource = resource\n",
    "        self.pipeline_id = pipeline_id\n",
    "        self.resource_type = resource_type\n",
    "        self.depends_on = depends_on\n",
    "        self.cluster = cluster\n",
    "        self.params = params\n",
    "\n",
    "class JobConfig():\n",
    "    def __init__(self, job_name, tasks):\n",
    "        self.job_name = job_name\n",
    "        self.tasks = tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1597a5d-1321-46bc-9088-dfa7a6ed6e7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _print_job_config():\n",
    "    \"\"\"\n",
    "    Renders the configuration of the job as HTML\n",
    "    \"\"\"\n",
    "    config = get_job_config()\n",
    "    \n",
    "    border_color = \"1px solid rgba(0, 0, 0, 0.25)\"\n",
    "    td_style = f\"white-space:nowrap; padding: 8px; border: 0; border-left: {border_color}; border-top: {border_color}\"\n",
    "    \n",
    "    html = f\"\"\"  \n",
    "    <p style=\"font-size: 16px\">Job Name: <span style=\"font-weight:bold\">{config.job_name}</span></p>\n",
    "    \n",
    "    <table style=\"width:100%; border-collapse: separate; border-spacing: 0; border-right: {border_color}; border-bottom: {border_color}; color: background-color: rgba(0, 0, 0, 0.8)\">\n",
    "        <tr>\n",
    "            <td style=\"{td_style}; background-color: rgba(245,245,245,1); width:1em\">Description</td>\n",
    "            <td style=\"{td_style}; background-color: rgba(245,245,245,1); width:8em\">Task Name</td>\n",
    "            <td style=\"{td_style}; background-color: rgba(245,245,245,1); width:11em\">Task Type</td>\n",
    "            <td style=\"{td_style}; background-color: rgba(245,245,245,1)\">Resource</td>\n",
    "            <td style=\"{td_style}; background-color: rgba(245,245,245,1)\">Depends On</td>\n",
    "            <td style=\"{td_style}; background-color: rgba(245,245,245,1)\">Parameters</td>\n",
    "        </tr>\n",
    "    \"\"\"\n",
    "    for i, task in enumerate(config.tasks):\n",
    "        html += f\"\"\"\n",
    "            <tr>\n",
    "                <td style=\"{td_style}\">Task #{i+1}:</td>\n",
    "                <td style=\"{td_style}\"><input type=\"text\" value=\"{task.name}\" style=\"width:100%; font-weight: bold\"></td>\n",
    "                <td style=\"{td_style}; font-weight: bold\">{task.resource_type}</td>\n",
    "                <td style=\"{td_style}; font-weight: bold\">{task.resource}</td>\n",
    "                <td style=\"{td_style}; font-weight: bold\">{\", \".join(task.depends_on)}</td>\n",
    "                <td style=\"{td_style}; font-weight: bold\">{task.params}</td>\n",
    "            </tr>\"\"\"\n",
    "        \n",
    "    html += \"\\n</table>\"\n",
    "    displayHTML(html)\n",
    "\n",
    "DA.print_job_config = _print_job_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e056694-2599-4660-80c0-6927e7663aef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_job():\n",
    "    \"\"\"\n",
    "    Creates the prescribed job.\n",
    "    \"\"\"\n",
    "    import re, json\n",
    "    from dbacademy.dbrest import DBAcademyRestClient\n",
    "    client = DBAcademyRestClient()\n",
    "\n",
    "    config = get_job_config()\n",
    "    print(f\"Creating the job {config.job_name}\")\n",
    "\n",
    "    # Delete the existing pipeline if it exists\n",
    "    client.jobs().delete_by_name(config.job_name, success_only=False)\n",
    "\n",
    "    course_name = re.sub(\"[^a-zA-Z0-9]\", \"-\", DA.course_name)\n",
    "    while \"--\" in course_name: course_name = course_name.replace(\"--\", \"-\")\n",
    "    \n",
    "    params = {\n",
    "        \"name\": f\"{config.job_name}\",\n",
    "        \"tags\": {\n",
    "            \"dbacademy.course\": course_name,\n",
    "            \"dbacademy.source\": course_name\n",
    "        },\n",
    "        \"email_notifications\": {},\n",
    "        \"timeout_seconds\": 7200,\n",
    "        \"max_concurrent_runs\": 1,\n",
    "        \"format\": \"MULTI_TASK\",\n",
    "        \"tasks\": [],\n",
    "        \"job_clusters\": [{\n",
    "            \"job_cluster_key\": \"shared_cluster\",\n",
    "            \"new_cluster\": {\n",
    "                \"num_workers\": 0,\n",
    "                \"spark_version\": f\"{client.clusters().get_current_spark_version()}\",\n",
    "                \"spark_conf\": { \"spark.master\": \"local[*]\" },\n",
    "            },\n",
    "        }]\n",
    "    }\n",
    "    \n",
    "    for task in config.tasks:\n",
    "        task_def = {\n",
    "            \"task_key\": task.name,\n",
    "        }\n",
    "        params.get(\"tasks\").append(task_def)\n",
    "        if task.cluster is not None: task_def[\"job_cluster_key\"] = task.cluster\n",
    "        \n",
    "        if task.pipeline_id is not None: \n",
    "            task_def[\"pipeline_task\"] = {\"pipeline_id\": task.pipeline_id}\n",
    "        else: \n",
    "            task_def[\"notebook_task\"] = {\n",
    "                \"notebook_path\": task.resource,\n",
    "                \"base_parameters\": task.params\n",
    "            }\n",
    "            \n",
    "        if len(task.depends_on) > 0:\n",
    "            task_def[\"depends_on\"] = list()\n",
    "            for key in task.depends_on: task_def[\"depends_on\"].append({\"task_key\":key})\n",
    "        \n",
    "    instance_pool_id = client.clusters().get_current_instance_pool_id()\n",
    "    cluster = params.get(\"job_clusters\")[0].get(\"new_cluster\")\n",
    "    if instance_pool_id:\n",
    "        cluster[\"instance_pool_id\"] = instance_pool_id\n",
    "    else:\n",
    "        node_type_id = client.clusters().get_current_node_type_id()\n",
    "        cluster[\"node_type_id\"] = node_type_id\n",
    "        \n",
    "    # print(json.dumps(params, indent=4))\n",
    "    \n",
    "    json_response = client.jobs().create(params)\n",
    "    job_id = json_response[\"job_id\"]\n",
    "    print(f\"Created job {job_id}\")\n",
    "    return job_id\n",
    "\n",
    "DA.create_job = _create_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78c6ba0a-d143-434e-9aff-47aa834916cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _start_job(job_id):\n",
    "    \"Starts the job and then blocks until it is TERMINATED or INTERNAL_ERROR\"\n",
    "\n",
    "    from dbacademy.dbrest import DBAcademyRestClient\n",
    "    client = DBAcademyRestClient()\n",
    "\n",
    "    run_id = client.jobs().run_now(job_id).get(\"run_id\")\n",
    "    response = client.runs().wait_for(run_id)\n",
    "    \n",
    "    state = response.get(\"state\").get(\"life_cycle_state\")\n",
    "    assert state in [\"TERMINATED\", \"INTERNAL_ERROR\", \"SKIPPED\"], f\"Expected final state: {state}\"\n",
    "    \n",
    "DA.start_job = _start_job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b48ebed1-be2c-4c7f-ad33-d6ec966aa079",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_date_lookup():\n",
    "    import time\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(f\"Creating date_lookup\", end=\"...\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE OR REPLACE TABLE date_lookup\n",
    "      SHALLOW CLONE delta.`{DA.hidden.datasets}/date-lookup`\n",
    "      LOCATION '{DA.paths.user_db}/date_lookup'\n",
    "    \"\"\")\n",
    "\n",
    "#     spark.sql(f\"\"\"\n",
    "#       CREATE TABLE date_lookup\n",
    "#       LOCATION '{DA.hidden.datasets}/date-lookup'\n",
    "#     \"\"\")\n",
    "\n",
    "    total = spark.read.table(\"date_lookup\").count()\n",
    "    assert total == 1096, f\"Expected 1,096 records, found {total:,} in date_lookup\"    \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f1c4bab8-48ca-4166-b342-9ea02965cbbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_gym_mac_logs():\n",
    "    import time\n",
    "    start = int(time.time())\n",
    "    print(f\"Creating gym_mac_logs\", end=\"...\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE OR REPLACE TABLE gym_mac_logs\n",
    "      SHALLOW CLONE delta.`{DA.hidden.datasets}/gym-mac-logs`\n",
    "      LOCATION '{DA.paths.user_db}/gym_mac_logs'\n",
    "    \"\"\")\n",
    "\n",
    "#     spark.sql(f\"\"\"\n",
    "#       CREATE TABLE gym_mac_logs\n",
    "#       LOCATION '{DA.hidden.datasets}/gym-mac-logs'\n",
    "#     \"\"\")\n",
    "\n",
    "    total = spark.read.table(\"gym_mac_logs\").count()\n",
    "    assert total == 314, f\"Expected 314 records, found {total:,} in gym_mac_logs\"\n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "\n",
    "DA.create_gym_mac_logs = _create_gym_mac_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "048ce74c-d9af-49d8-aca1-1fd09fd33125",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_user_lookup():\n",
    "    import time\n",
    "    \n",
    "    start = int(time.time())\n",
    "    print(f\"Creating user_lookup\", end=\"...\")\n",
    "\n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE OR REPLACE TABLE user_lookup\n",
    "      SHALLOW CLONE delta.`{DA.hidden.datasets}/user-lookup`\n",
    "      LOCATION '{DA.paths.user_db}/user_lookup'\n",
    "    \"\"\")\n",
    "\n",
    "#     spark.sql(f\"\"\"\n",
    "#       CREATE TABLE user_lookup\n",
    "#       LOCATION '{DA.hidden.datasets}/user-lookup'\n",
    "#     \"\"\")\n",
    "\n",
    "    total = spark.sql(f\"SELECT * FROM user_lookup\").count()\n",
    "    assert total == 100, f\"Expected 100 records, found {total:,} in user_lookup\"    \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")    \n",
    "    \n",
    "DA.create_user_lookup = _create_user_lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "429b9eed-80cc-41ac-9d0d-4d2a8d4cd37e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class DailyStreamingFactory:\n",
    "    def __init__(self, target_dir, starting_batch=1, max_batch=16):\n",
    "        self.target_dir = target_dir\n",
    "        self.max_batch = max_batch\n",
    "        self.batch = starting_batch\n",
    "    \n",
    "    def load_through(self, max_batch):\n",
    "        import time\n",
    "        \n",
    "        start = int(time.time())\n",
    "        print(f\"Loading all batches from #{self.batch} through #{max_batch} to the daily stream\", end=\"...\")\n",
    "        \n",
    "        total = self.load_batch(self.batch, max_batch)\n",
    "        self.batch = max_batch+1\n",
    "        \n",
    "        print(f\"({int(time.time())-start} seconds, {total:,} records)\")\n",
    "    \n",
    "    def load(self, continuous=False):\n",
    "        import time\n",
    "        \n",
    "        start = int(time.time())\n",
    "        \n",
    "        if self.batch > self.max_batch:\n",
    "            total = 0\n",
    "            print(\"Data source exhausted\", end=\"...\")\n",
    "            \n",
    "        elif continuous == True:\n",
    "            print(\"Loading all batches to the daily stream\", end=\"...\")\n",
    "            total = self.load_batch(self.batch, self.max_batch)\n",
    "            self.batch = self.max_batch+1\n",
    "            \n",
    "        else:\n",
    "            print(f\"Loading batch #{self.batch} to the daily stream\", end=\"...\")\n",
    "            total = self.load_batch(self.batch, self.batch)\n",
    "            self.batch += 1\n",
    "            \n",
    "        print(f\"({int(time.time())-start} seconds, {total:,} records)\")\n",
    "    \n",
    "    def load_batch(self, min_batch, max_batch):\n",
    "        from pyspark.sql import functions as F\n",
    "\n",
    "        # Refactoring didn't improve performance        \n",
    "        # df = spark.read.load(f\"{DA.hidden.datasets}/bronze\")\n",
    "        # if min_batch == 1 and max_batch == 1:\n",
    "        #     # Day #1 and only day #1\n",
    "        #     # Include all records from 12-01 and back\n",
    "        #     df = df.filter(\"date <= '2019-12-01'\")\n",
    "        # elif min_batch == 1:\n",
    "        #     # Day #1 through max_batch\n",
    "        #     # Include all records from max_batch and back\n",
    "        #     df = df.filter(f\"date <= '2019-12-{max_batch:02d}'\")\n",
    "        # elif min_batch == max_batch:\n",
    "        #     # Just one day out of the set, but not day #1\n",
    "        #     df = df.filter(f\"date = '2019-12-{max_batch:02d}'\")\n",
    "        # else:\n",
    "        #     # Range of dates, but not day #1\n",
    "        #     df = df.filter(f\"date >= '2019-12-{min_batch:02d}' and date <= '2019-12-{max_batch:02d}'\")\n",
    "\n",
    "        # Crippling the predicate here. Experiments yeild no better result short\n",
    "        # of restructuring the underlying datsets to support a date-based predicate\n",
    "        df = (spark.read\n",
    "                   .load(f\"{DA.hidden.datasets}/bronze\")\n",
    "                   .withColumn(\"day\", F.when(F.col(\"date\") <= '2019-12-01', 1).otherwise(F.dayofmonth(\"date\")))\n",
    "                   .filter(F.col(\"day\") >= min_batch)\n",
    "                   .filter(F.col(\"day\") <= max_batch)\n",
    "                   .drop(\"date\", \"week_part\", \"day\"))\n",
    "              \n",
    "        df.write.mode(\"append\").format(\"json\").save(self.target_dir)\n",
    "        return df.count()\n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7bf44b12-5ae4-4048-a0ae-7f8d9cfd9aed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def init_source_daily():\n",
    "    DA.paths.source_daily = f\"{DA.paths.working_dir}/streams/daily\"\n",
    "    DA.daily_stream = DailyStreamingFactory(DA.paths.source_daily)\n",
    "    \n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2420dfe0-0672-4eeb-bd71-4d4057e0b2d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_partitioned_bronze_table():\n",
    "    import time\n",
    "    \n",
    "    start = int(time.time())\n",
    "    print(f\"Creating bronze\", end=\"...\")\n",
    "    \n",
    "    (spark.read\n",
    "          .load(f\"{DA.hidden.datasets}/bronze\")\n",
    "          .write\n",
    "          .partitionBy(\"topic\", \"week_part\")\n",
    "          .option(\"path\", f\"{DA.paths.user_db}/bronze\")\n",
    "          .saveAsTable(\"bronze\"))\n",
    "\n",
    "    total = spark.read.table(\"bronze\").count()\n",
    "    assert total == 10841978, f\"Expected 10,841,978 records, found {total:,} in bronze\"    \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "\n",
    "DA.create_partitioned_bronze_table = _create_partitioned_bronze_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a2de577-ad35-4f24-887d-2456997cb3d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _create_bronze_table():\n",
    "    import time\n",
    "    \n",
    "    start = int(time.time())\n",
    "    print(f\"Creating bronze\", end=\"...\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "      CREATE TABLE bronze\n",
    "      SHALLOW CLONE delta.`{DA.hidden.datasets}/bronze`\n",
    "      LOCATION '{DA.paths.user_db}/bronze'\n",
    "    \"\"\") \n",
    "    \n",
    "    total = spark.read.table(\"bronze\").count()\n",
    "    assert total == 10841978, f\"Expected 10,841,978 records, found {total:,} in bronze\"    \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "DA.create_bronze_table = _create_bronze_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a307b99c-f857-4b69-9736-451eae2658c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# This is the solution from lesson 2.03 and is included\n",
    "# here to fast-forward the student to this stage\n",
    "def _process_bronze():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(f\"Processing the bronze table from the daily stream\", end=\"...\")\n",
    "        \n",
    "    schema = \"key BINARY, value BINARY, topic STRING, partition LONG, offset LONG, timestamp LONG\"\n",
    "    date_lookup_df = spark.table(\"date_lookup\").select(\"date\", \"week_part\")\n",
    "\n",
    "    def execute_stream():\n",
    "        query = (spark.readStream\n",
    "                      .format(\"cloudFiles\")\n",
    "                      .schema(schema)\n",
    "                      .option(\"cloudFiles.format\", \"json\")\n",
    "                      .load(DA.paths.source_daily)\n",
    "                      .join(F.broadcast(date_lookup_df), F.to_date((F.col(\"timestamp\")/1000).cast(\"timestamp\")) == F.col(\"date\"), \"left\")\n",
    "                      .writeStream\n",
    "                      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/bronze\")\n",
    "                      .partitionBy(\"topic\", \"week_part\")\n",
    "                      .option(\"path\", f\"{DA.paths.user_db}/bronze\")\n",
    "                      .trigger(availableNow=True)\n",
    "                      .table(\"bronze\"))\n",
    "        query.awaitTermination()\n",
    "    \n",
    "    # The cluster is going to cache the state of the stream and it will be wrong.\n",
    "    # But it will also invalidate that cache allowing us to try again.\n",
    "    try: execute_stream()\n",
    "    except AnalysisException: execute_stream()\n",
    "    \n",
    "    total = spark.read.table(\"bronze\").count()\n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "DA.process_bronze = _process_bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd406d0a-6886-4280-bdba-cfbbeb457cc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _process_heart_rate_silver_v0():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(\"Processing the heart_rate_silver table\", end=\"...\")\n",
    "\n",
    "    class Upsert:\n",
    "        def __init__(self, sql_query, update_temp=\"stream_updates\"):\n",
    "            self.sql_query = sql_query\n",
    "            self.update_temp = update_temp \n",
    "\n",
    "        def upsertToDelta(self, microBatchDF, batch):\n",
    "            microBatchDF.createOrReplaceTempView(self.update_temp)\n",
    "            microBatchDF._jdf.sparkSession().sql(self.sql_query)\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS heart_rate_silver (device_id LONG, time TIMESTAMP, heartrate DOUBLE) USING DELTA\")\n",
    "    \n",
    "    streamingMerge=Upsert(\"\"\"\n",
    "      MERGE INTO heart_rate_silver a\n",
    "      USING stream_updates b\n",
    "      ON a.device_id=b.device_id AND a.time=b.time\n",
    "      WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    def execute_stream():\n",
    "        (spark.readStream\n",
    "              .table(\"bronze\")\n",
    "              .filter(\"topic = 'bpm'\")\n",
    "              .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n",
    "              .select(\"v.*\")\n",
    "              .withWatermark(\"time\", \"30 seconds\")\n",
    "              .dropDuplicates([\"device_id\", \"time\"])\n",
    "              .writeStream\n",
    "              .foreachBatch(streamingMerge.upsertToDelta)\n",
    "              .outputMode(\"update\")\n",
    "              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/heart_rate\")\n",
    "              .trigger(availableNow=True)\n",
    "              .start()\n",
    "              .awaitTermination())\n",
    "    \n",
    "    # The cluster is going to cache the state of the stream and it will be wrong.\n",
    "    # But it will also invalidate that cache allowing us to try again.\n",
    "    try: execute_stream()\n",
    "    except AnalysisException: execute_stream()\n",
    "    \n",
    "    total = spark.read.table(\"heart_rate_silver\").count() \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "\n",
    "DA.process_heart_rate_silver_v0 = _process_heart_rate_silver_v0\n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abb92b27-f17f-49a3-97dc-1ee594caa194",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _process_heart_rate_silver():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(\"Processing the heart_rate_silver table\", end=\"...\")\n",
    "\n",
    "    class Upsert:\n",
    "        def __init__(self, sql_query, update_temp=\"stream_updates\"):\n",
    "            self.sql_query = sql_query\n",
    "            self.update_temp = update_temp \n",
    "\n",
    "        def upsertToDelta(self, microBatchDF, batch):\n",
    "            microBatchDF.createOrReplaceTempView(self.update_temp)\n",
    "            microBatchDF._jdf.sparkSession().sql(self.sql_query)\n",
    "    \n",
    "    spark.sql(\"CREATE TABLE IF NOT EXISTS heart_rate_silver (device_id LONG, time TIMESTAMP, heartrate DOUBLE, bpm_check STRING) USING DELTA\")\n",
    "    \n",
    "    streamingMerge=Upsert(\"\"\"\n",
    "      MERGE INTO heart_rate_silver a\n",
    "      USING stream_updates b\n",
    "      ON a.device_id=b.device_id AND a.time=b.time\n",
    "      WHEN NOT MATCHED THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    def execute_stream():\n",
    "        (spark.readStream\n",
    "              .table(\"bronze\")\n",
    "              .filter(\"topic = 'bpm'\")\n",
    "              .select(F.from_json(F.col(\"value\").cast(\"string\"), \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\").alias(\"v\"))\n",
    "              .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\").otherwise(\"OK\").alias(\"bpm_check\"))\n",
    "              .withWatermark(\"time\", \"30 seconds\")\n",
    "              .dropDuplicates([\"device_id\", \"time\"])\n",
    "              .writeStream\n",
    "              .foreachBatch(streamingMerge.upsertToDelta)\n",
    "              .outputMode(\"update\")\n",
    "              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/heart_rate\")\n",
    "              .trigger(availableNow=True)\n",
    "              .start()\n",
    "              .awaitTermination())    \n",
    "    \n",
    "    # The cluster is going to cache the state of the stream and it will be wrong.\n",
    "    # But it will also invalidate that cache allowing us to try again.\n",
    "    try: execute_stream()\n",
    "    except AnalysisException: execute_stream()\n",
    "        \n",
    "    total = spark.read.table(\"heart_rate_silver\").count() \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "\n",
    "DA.process_heart_rate_silver = _process_heart_rate_silver\n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e5eb8403-8922-4a0d-9af3-e646999c253e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _process_workouts_silver():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(\"Processing the workouts_silver table\", end=\"...\")\n",
    "    \n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS workouts_silver (user_id INT, workout_id INT, time TIMESTAMP, action STRING, session_id INT) USING DELTA\")\n",
    "    \n",
    "    class Upsert:\n",
    "        def __init__(self, query, update_temp=\"stream_updates\"):\n",
    "            self.query = query\n",
    "            self.update_temp = update_temp \n",
    "\n",
    "        def upsertToDelta(self, microBatchDF, batch):\n",
    "            microBatchDF.createOrReplaceTempView(self.update_temp)\n",
    "            microBatchDF._jdf.sparkSession().sql(self.query)\n",
    "    \n",
    "    sql_query = \"\"\"\n",
    "        MERGE INTO workouts_silver a\n",
    "        USING workout_updates b\n",
    "        ON a.user_id=b.user_id AND a.time=b.time\n",
    "        WHEN NOT MATCHED THEN INSERT *\n",
    "        \"\"\"\n",
    "\n",
    "    streamingMerge=Upsert(sql_query, \"workout_updates\")\n",
    "    \n",
    "    def execute_stream():\n",
    "        (spark.readStream\n",
    "              .option(\"ignoreDeletes\", True)\n",
    "              .table(\"bronze\")\n",
    "              .filter(\"topic = 'workout'\")\n",
    "              .select(F.from_json(F.col(\"value\").cast(\"string\"), \"user_id INT, workout_id INT, timestamp FLOAT, action STRING, session_id INT\").alias(\"v\"))\n",
    "              .select(\"v.*\")\n",
    "              .select(\"user_id\", \"workout_id\", F.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"), \"action\", \"session_id\")\n",
    "              .withWatermark(\"time\", \"30 seconds\")\n",
    "              .dropDuplicates([\"user_id\", \"time\"])\n",
    "              .writeStream\n",
    "              .foreachBatch(streamingMerge.upsertToDelta)\n",
    "              .outputMode(\"update\")\n",
    "              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/workouts\")\n",
    "              .queryName(\"workouts_silver\")\n",
    "              .trigger(availableNow=True)\n",
    "              .start()\n",
    "              .awaitTermination())\n",
    "        \n",
    "    # The cluster is going to cache the state of the stream and it will be wrong.\n",
    "    # But it will also invalidate that cache allowing us to try again.\n",
    "    try: execute_stream()\n",
    "    except AnalysisException: execute_stream()\n",
    "    \n",
    "    total = spark.read.table(\"workouts_silver\").count() \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "DA.process_workouts_silver = _process_workouts_silver\n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb986f40-39bb-481d-8ccd-9e150d58ba0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _process_completed_workouts():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(\"Processing the completed_workouts table\", end=\"...\")\n",
    "\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW TEMP_completed_workouts AS (\n",
    "          SELECT a.user_id, a.workout_id, a.session_id, a.start_time start_time, b.end_time end_time, a.in_progress AND (b.in_progress IS NULL) in_progress\n",
    "          FROM (\n",
    "            SELECT user_id, workout_id, session_id, time start_time, null end_time, true in_progress\n",
    "            FROM workouts_silver\n",
    "            WHERE action = \"start\") a\n",
    "          LEFT JOIN (\n",
    "            SELECT user_id, workout_id, session_id, null start_time, time end_time, false in_progress\n",
    "            FROM workouts_silver\n",
    "            WHERE action = \"stop\") b\n",
    "          ON a.user_id = b.user_id AND a.session_id = b.session_id\n",
    "        )\n",
    "    \"\"\")\n",
    "    \n",
    "    (spark.table(\"TEMP_completed_workouts\").write.mode(\"overwrite\").saveAsTable(\"completed_workouts\"))\n",
    "    \n",
    "    total = spark.read.table(\"completed_workouts\").count() \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "DA.process_completed_workouts = _process_completed_workouts\n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4778ff2-a1d8-4c17-ac82-f387a2b6323b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _process_workout_bpm():\n",
    "    import time\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    \n",
    "    start = int(time.time())\n",
    "    print(\"Processing the workout_bpm table\", end=\"...\")\n",
    "\n",
    "    spark.readStream.table(\"heart_rate_silver\").createOrReplaceTempView(\"TEMP_heart_rate_silver\")\n",
    "    \n",
    "    spark.sql(\"\"\"\n",
    "        SELECT d.user_id, d.workout_id, d.session_id, time, heartrate\n",
    "        FROM TEMP_heart_rate_silver c\n",
    "        INNER JOIN (\n",
    "          SELECT a.user_id, b.device_id, workout_id, session_id, start_time, end_time\n",
    "          FROM completed_workouts a\n",
    "          INNER JOIN user_lookup b\n",
    "          ON a.user_id = b.user_id) d\n",
    "        ON c.device_id = d.device_id AND time BETWEEN start_time AND end_time\n",
    "        WHERE c.bpm_check = 'OK'\"\"\").createOrReplaceTempView(\"TEMP_workout_bpm\")\n",
    "    \n",
    "    def execute_stream():\n",
    "        (spark.table(\"TEMP_workout_bpm\")\n",
    "            .writeStream\n",
    "            .format(\"delta\")\n",
    "            .outputMode(\"append\")\n",
    "            .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/workout_bpm\")\n",
    "            .option(\"path\", f\"{DA.paths.user_db}/workout_bpm\")\n",
    "            .trigger(availableNow=True)\n",
    "            .table(\"workout_bpm\")\n",
    "            .awaitTermination())\n",
    "    \n",
    "    # The cluster is going to cache the state of the stream and it will be wrong.\n",
    "    # But it will also invalidate that cache allowing us to try again.\n",
    "    try: execute_stream()\n",
    "    except AnalysisException: execute_stream()\n",
    "\n",
    "    total = spark.read.table(\"workout_bpm\").count() \n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "DA.process_workout_bpm = _process_workout_bpm\n",
    "\n",
    "None # Suppressing Output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4856bc4d-5a41-4115-b727-f1b7847e8712",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def batch_rank_upsert(microBatchDF, batchId):\n",
    "    from pyspark.sql.window import Window\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    window = Window.partitionBy(\"alt_id\").orderBy(F.col(\"updated\").desc())\n",
    "    \n",
    "    (microBatchDF\n",
    "        .filter(F.col(\"update_type\").isin([\"new\", \"update\"]))\n",
    "        .withColumn(\"rank\", F.rank().over(window))\n",
    "        .filter(\"rank == 1\")\n",
    "        .drop(\"rank\")\n",
    "        .createOrReplaceTempView(\"ranked_updates\"))\n",
    "    \n",
    "    microBatchDF._jdf.sparkSession().sql(\"\"\"\n",
    "        MERGE INTO users u\n",
    "        USING ranked_updates r\n",
    "        ON u.alt_id=r.alt_id\n",
    "        WHEN MATCHED AND u.updated < r.updated\n",
    "          THEN UPDATE SET *\n",
    "        WHEN NOT MATCHED\n",
    "          THEN INSERT *\n",
    "    \"\"\")\n",
    "\n",
    "    (microBatchDF\n",
    "        .filter(\"update_type = 'delete'\")\n",
    "        .select(\"alt_id\", \n",
    "                F.col(\"updated\").alias(\"requested\"), \n",
    "                F.date_add(\"updated\", 30).alias(\"deadline\"), \n",
    "                F.lit(\"requested\").alias(\"status\"))\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"append\")\n",
    "        .option(\"txnVersion\", batchId)\n",
    "        .option(\"txnAppId\", \"batch_rank_upsert\")\n",
    "        .option(\"path\", f\"{DA.paths.user_db}/delete_requests\")\n",
    "        .saveAsTable(\"delete_requests\"))\n",
    "    \n",
    "def _process_users():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "    from pyspark.sql.utils import AnalysisException\n",
    "    \n",
    "    start = int(time.time())\n",
    "    print(f\"Processing the users table\", end=\"...\")\n",
    "\n",
    "    spark.sql(f\"CREATE TABLE IF NOT EXISTS users (alt_id STRING, dob DATE, sex STRING, gender STRING, first_name STRING, last_name STRING, street_address STRING, city STRING, state STRING, zip INT, updated TIMESTAMP) USING DELTA\")\n",
    "    \n",
    "    schema = \"\"\"\n",
    "        user_id LONG, \n",
    "        update_type STRING, \n",
    "        timestamp FLOAT, \n",
    "        dob STRING, \n",
    "        sex STRING, \n",
    "        gender STRING, \n",
    "        first_name STRING, \n",
    "        last_name STRING, \n",
    "        address STRUCT<\n",
    "            street_address: STRING, \n",
    "            city: STRING, \n",
    "            state: STRING, \n",
    "            zip: INT\n",
    "    >\"\"\"\n",
    "    \n",
    "    def execute_stream():\n",
    "        (spark.readStream\n",
    "            .table(\"bronze\")\n",
    "            .filter(\"topic = 'user_info'\")\n",
    "            .dropDuplicates()\n",
    "            .select(F.from_json(F.col(\"value\").cast(\"string\"), schema).alias(\"v\")).select(\"v.*\")\n",
    "            .select(F.sha2(F.concat(F.col(\"user_id\"), F.lit(\"BEANS\")), 256).alias(\"alt_id\"),\n",
    "                F.col('timestamp').cast(\"timestamp\").alias(\"updated\"),\n",
    "                F.to_date('dob','MM/dd/yyyy').alias('dob'),\n",
    "                'sex', 'gender','first_name','last_name',\n",
    "                'address.*', \"update_type\")\n",
    "            .writeStream\n",
    "            .foreachBatch(batch_rank_upsert)\n",
    "            .outputMode(\"update\")\n",
    "            .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/users\")\n",
    "            .trigger(availableNow=True)\n",
    "            .start()\n",
    "            .awaitTermination())    \n",
    "\n",
    "    # The cluster is going to cache the state of the stream and it will be wrong.\n",
    "    # But it will also invalidate that cache allowing us to try again.\n",
    "    try: execute_stream()\n",
    "    except AnalysisException: execute_stream()\n",
    "        \n",
    "    print(f\"({int(time.time())-start} seconds)\")\n",
    "\n",
    "#     total = spark.read.table(\"ranked_updates\").count()\n",
    "#     print(f\"...ranked_updates: {total} records)\")\n",
    "\n",
    "    total = spark.read.table(\"delete_requests\").count()\n",
    "    print(f\"...delete_requests: {total} records)\")\n",
    "\n",
    "    total = spark.read.table(\"users\").count()\n",
    "    print(f\"...users: {total} records)\")\n",
    "    \n",
    "DA.process_users = _process_users\n",
    "    \n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2dc9dbbd-dd7d-45bb-abd3-cfdf7cef22f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def age_bins(dob_col):\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    age_col = F.floor(F.months_between(F.current_date(), dob_col)/12).alias(\"age\")\n",
    "    return (F.when((age_col < 18), \"under 18\")\n",
    "            .when((age_col >= 18) & (age_col < 25), \"18-25\")\n",
    "            .when((age_col >= 25) & (age_col < 35), \"25-35\")\n",
    "            .when((age_col >= 35) & (age_col < 45), \"35-45\")\n",
    "            .when((age_col >= 45) & (age_col < 55), \"45-55\")\n",
    "            .when((age_col >= 55) & (age_col < 65), \"55-65\")\n",
    "            .when((age_col >= 65) & (age_col < 75), \"65-75\")\n",
    "            .when((age_col >= 75) & (age_col < 85), \"75-85\")\n",
    "            .when((age_col >= 85) & (age_col < 95), \"85-95\")\n",
    "            .when((age_col >= 95), \"95+\")\n",
    "            .otherwise(\"invalid age\").alias(\"age\"))\n",
    "\n",
    "def _process_user_bins():\n",
    "    import time\n",
    "    from pyspark.sql import functions as F\n",
    "\n",
    "    start = int(time.time())\n",
    "    print(f\"Processing user_bins table\", end=\"...\")\n",
    "    \n",
    "    (spark.table(\"users\")\n",
    "         .join(\n",
    "            spark.table(\"user_lookup\")\n",
    "                .select(\"alt_id\", \"user_id\"), \n",
    "            [\"alt_id\"], \n",
    "            \"left\")\n",
    "        .select(\"user_id\", \n",
    "                age_bins(F.col(\"dob\")),\n",
    "                \"gender\", \n",
    "                \"city\", \n",
    "                \"state\")\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .option(\"path\", f\"{DA.paths.user_db}/user_bins\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(\"user_bins\"))\n",
    "    \n",
    "    total = spark.read.table(\"user_bins\").count()\n",
    "    print(f\"({int(time.time())-start} seconds / {total:,} records)\")\n",
    "    \n",
    "\n",
    "DA.process_user_bins = _process_user_bins    \n",
    "\n",
    "None # Suppressing Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0b20d3b9-35a5-4f46-ad80-a17ab1e76188",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def _validate_count_less_than(df, expected):\n",
    "    from pyspark.sql.dataframe import DataFrame    \n",
    "    \n",
    "    if type(df) == str: df = spark.read.table(df)\n",
    "    elif type(df) != DataFrame: raise ValueError(f\"Invalid parameter type ({type(df)}), expected str or DataFrame\")\n",
    "    \n",
    "    total = df.count()\n",
    "    assert total < expected, f\"Expected less than {expected:,} records, found {total:,} records.\"    \n",
    "    print(f\"Found {total:,} records as expected.\")\n",
    "    return total\n",
    "\n",
    "def _validate_count_greater_than(df, expected):\n",
    "    from pyspark.sql.dataframe import DataFrame    \n",
    "    \n",
    "    if type(df) == str: df = spark.read.table(df)\n",
    "    elif type(df) != DataFrame: raise ValueError(f\"Invalid parameter type ({type(df)}), expected str or DataFrame\")\n",
    "    \n",
    "    total = df.count()\n",
    "    assert total > expected, f\"Expected less than {expected:,} records, found {total:,} records.\"    \n",
    "    print(f\"Found {total:,} records as expected.\")\n",
    "    return total\n",
    "\n",
    "def _validate_count(df, expected):\n",
    "    from pyspark.sql.dataframe import DataFrame    \n",
    "    \n",
    "    if type(df) == str: df = spark.read.table(df)\n",
    "    elif type(df) != DataFrame: raise ValueError(f\"Invalid parameter type ({type(df)}), expected str or DataFrame\")\n",
    "    \n",
    "    return _validate_total(expected, df.count(), \"records\")\n",
    "\n",
    "def _validate_length(what, expected, name):\n",
    "    return _validate_total(expected, len(what), name)\n",
    "\n",
    "def _validate_total(expected, actual, name):\n",
    "    assert expected == actual, f\"Expected {expected:,} {name}, found {actual:,} {name}\"\n",
    "    print(f\"Found {actual:,} {name} as expected.\")\n",
    "    return actual\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "_utility-functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}