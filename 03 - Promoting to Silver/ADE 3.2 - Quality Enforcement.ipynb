{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4994ec1a-b40b-486a-bcba-3b51945d4479",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7566a21-addc-4bd4-bf24-5b4026b5d7bd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Quality Enforcement\n",
    "\n",
    "One of the main motivations for using Delta Lake to store data is that you can provide guarantees on the quality of your data. While schema enforcement is automatic, additional quality checks can be helpful to ensure that only data that meets your expectations makes it into your Lakehouse.\n",
    "\n",
    "This notebook will review a few approaches to quality enforcement. Some of these are Databricks-specific features, while others are general design principles.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Add check constraints to Delta tables\n",
    "- Describe and implement a quarantine table\n",
    "- Apply logic to add data quality tags to Delta tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "efdfeef7-af98-4e2e-aa8d-b1e8ba14b35c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c1507b1-a5c6-41ec-b75f-39092601d5c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Constraints\n",
    "\n",
    "Databricks allows <a href=\"https://docs.databricks.com/delta/delta-constraints.html\" target=\"_blank\">table constraints</a> to be set on Delta tables.\n",
    "\n",
    "Table constraints apply boolean filters to columns within a table and prevent data that does not fulfill these constraints from being written."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d41c0aca-5924-475a-acb7-66d219197f12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start by looking at our existing tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4f3fc47b-dc59-4361-8c18-f14260b1b581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SHOW TABLES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d21adf64-7aac-476f-9fce-59620439259c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "If these exist, table constraints will be listed under the **`properties`** of the extended table description."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05757df4-1328-47cc-95bd-7b9e0fcf8512",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED heart_rate_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "004714ff-ff92-4efa-9365-1c6c38295b34",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When defining a constraint, be sure to give it a human-readable name. (Note that names are not case sensitive.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e035360-ee45-4ffe-a9cf-eab8841df5ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE heart_rate_silver ADD CONSTRAINT date_within_range CHECK (time > '2017-01-01');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66e9a5e8-c56e-43bf-aecf-a99e8a0e7a28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "None of the existing data in our table violated this constraint. Both the name and the actual check are displayed in the **`properties`** field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4d5444a6-cec2-4edc-a911-008213b04f03",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED heart_rate_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a49406e9-da3a-44d1-9f41-0c038a759a08",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "But what happens if the conditions of the constraint aren't met?\n",
    "\n",
    "We know that some of our devices occasionally send negative **`bpm`** recordings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a1d8bde3-1e51-4fea-ab64-fda02edd3446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*) FROM heart_rate_silver\n",
    "WHERE heartrate <= 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "95076823-b44b-47e0-a0cb-55abeb9e2f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Delta Lake will prevent us from applying a constraint that existing records violate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "430c1619-df6c-4028-a8d1-ff94b2cfc40c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "try:\n",
    "    spark.sql(\"ALTER TABLE heart_rate_silver ADD CONSTRAINT validbpm CHECK (heartrate > 0);\")\n",
    "    raise Exception(\"Expected failure\")\n",
    "\n",
    "except pyspark.sql.utils.AnalysisException as e:\n",
    "    print(\"Failed as expected...\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "087fd8ce-74e8-4b3a-a66c-57346b04754b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Notice below how we failed to applied the constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aed73fc-8dea-4cb3-9f50-6ebbea20a84f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE EXTENDED heart_rate_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca60c2ed-22f8-445d-8daa-a1d0d7f44e1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "How do we deal with this? \n",
    "\n",
    "We could manually delete offending records and then set the check constraint, or set the check constraint before processing data from our bronze table.\n",
    "\n",
    "However, if we set a check constraint and a batch of data contains records that violate it, the job will fail and we'll throw an error.\n",
    "\n",
    "If our goal is to identify bad records but keep streaming jobs running, we'll need a different solution.\n",
    "\n",
    "One idea would be to quarantine invalid records.\n",
    "\n",
    "Note that if you need to remove a constraint from a table, the following code would be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b174ff21-9994-4bf1-b732-b949e3a6c5c4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "ALTER TABLE heart_rate_silver DROP CONSTRAINT validbpm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cce84c70-f171-467e-904e-c58a13d4d3ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Quarantining\n",
    "\n",
    "The idea of quarantining is that bad records will be written to a separate location.\n",
    "\n",
    "This allows good data to processed efficiently, while additional logic and/or manual review of erroneous records can be defined and executed away from the main pipeline.\n",
    "\n",
    "Assuming that records can be successfully salvaged, they can be easily backfilled into the silver table they were deferred from.\n",
    "\n",
    "Here, we'll implement quarantining by performing writes to two separate tables within a **`foreachBatch`** custom writer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8031c2eb-c8b0-484c-a58d-e5e0ceaa4fd1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start by creating a table with the correct schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b15505a-56ee-4ae5-a6b6-5ae816915c22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS bpm_quarantine\n",
    "    (device_id LONG, time TIMESTAMP, heartrate DOUBLE)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.user_db}/bpm_quarantine'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96213f07-2508-4f8b-b08d-3ecbe8c85dd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "With Structured Streaming operations, writing to an additional table can be accomplished within **`foreachBatch`** logic.\n",
    "\n",
    "Below, we'll update the logic to add filters at the appropriate locations.\n",
    "\n",
    "For simplicity, we won't check for duplicate records as we insert data into the quarantine table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aef279d8-7b99-4301-9dd9-a9004a906003",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "MERGE INTO heart_rate_silver a\n",
    "USING stream_updates b\n",
    "ON a.device_id=b.device_id AND a.time=b.time\n",
    "WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\"\n",
    "\n",
    "class Upsert:\n",
    "    def __init__(self, query, update_temp=\"stream_updates\"):\n",
    "        self.query = query\n",
    "        self.update_temp = update_temp \n",
    "        \n",
    "    def upsert_to_delta(self, micro_batch_df, batch):\n",
    "        micro_batch_df.filter(\"heartrate\" > 0).createOrReplaceTempView(self.update_temp)\n",
    "        micro_batch_df._jdf.sparkSession().sql(self.query)\n",
    "        micro_batch_df.filter(\"heartrate\" <= 0).write.format(\"delta\").mode(\"append\").saveAsTable(\"bpm_quarantine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "acb413d4-3ba4-4a5f-acc6-b7a746399a45",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that within the **`foreachBatch`** logic, the DataFrame operations are treating the data in each batch as if it's static rather than streaming.\n",
    "\n",
    "As such, we use the **`write`** syntax instead of **`writeStream`**.\n",
    "\n",
    "This also means that our exactly-once guarantees are relaxed. In our example above, we have two ACID transactions:\n",
    "1. Our SQL query executes to run an insert-only merge to avoid writing duplicate records to our silver table.\n",
    "2. We write a microbatch of records with negative heartrates to the **`bpm_quarantine`** table\n",
    "\n",
    "If our job fails after our first transaction completes but before the second completes, we will re-execute the full microbatch logic on job restart.\n",
    "\n",
    "However, because our insert-only merge already prevents duplicate records from being saved to our table, this will not result in any data corruption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "93c61981-d863-4489-9a5e-b4fbd3839eda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "## Flagging\n",
    "To avoid multiple writes and managing multiple tables, you may choose to implement a flagging system to warn about violations while avoiding job failures.\n",
    "\n",
    "Flagging is a low touch solution with little overhead.\n",
    "\n",
    "These flags can easily be leveraged by filters in downstream queries to isolate bad data.\n",
    "\n",
    "**`case`** / **`when`** logic makes this easy.\n",
    "\n",
    "Run the following cell to see the compiled Spark SQL from the PySpark code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1e8c623b-2f20-4965-855b-822a8c44bb16",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "F.when(F.col(\"heartrate\") <= 0, \"Negative BPM\").otherwise(\"OK\").alias(\"bpm_check\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c81b587-de3c-46c0-92c6-22ece5574897",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Here, we'll just insert this logic as an additional transformation on a batch read of our bronze data to preview the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b83638e-16bc-409e-a27e-90bf80834d02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "deduped_df = (spark.read\n",
    "                  .table(\"bronze\")\n",
    "                  .filter(\"topic = 'bpm'\")\n",
    "                  .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                  .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\")\n",
    "                                  .otherwise(\"OK\")\n",
    "                                  .alias(\"bpm_check\"))\n",
    "                  .dropDuplicates([\"device_id\", \"time\"]))\n",
    "\n",
    "display(deduped_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66751527-0070-434d-a900-da57be40908f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9218b7b0-0c09-45bc-827e-72402ffe7e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2111d110-cc99-41b8-bbe4-f9edd55ccb0d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ADE 3.2 - Quality Enforcement",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}