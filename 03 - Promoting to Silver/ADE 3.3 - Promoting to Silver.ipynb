{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78e09378-39c9-4955-8165-7d68e357bff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "78b840bf-d3ea-4ad4-8d23-df99c8eea4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Promoting to Silver\n",
    "\n",
    "Here we'll pull together the concepts of streaming from Delta Tables, deduplication, and quality enforcement to finalize our approach to our silver table.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/ade/ADE_arch_heartrate_silver.png\" width=\"60%\" />\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, students will be able to:\n",
    "- Apply table constraints to Delta Lake tables\n",
    "- Use flagging to identify records failing to meet certain conditions\n",
    "- Apply de-duplication within an incremental microbatch\n",
    "- Use **`MERGE`** to avoid inserting duplicate records to a Delta Lake table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4bc3aca-b9a8-4669-9d47-d9a6f4f4cfdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bed07a2d-a196-4f43-b09d-6f381b8758fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b254c8b0-331e-4afd-8bce-df9c451ccbf4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Start by creating our **`heart_rate_silver`** table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "754bd3b0-46f3-45ac-acb8-ed4f755a224e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE TABLE IF NOT EXISTS heart_rate_silver\n",
    "  (device_id LONG, time TIMESTAMP, heartrate DOUBLE, bpm_check STRING)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.user_db}/heart_rate_silver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7044c868-b661-48c2-816f-7cd947c40e3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Table Constraint\n",
    "Add a table constraint before inserting data. Name this constraint **`dateWithinRange`** and make sure that the time is greater than January 1, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55f2238e-60d9-4add-84dc-24ae0600274c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- TODO\n",
    "ALTER TABLE -- <FILL-IN>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1158aa50-5233-40f3-b09e-70d7815b803f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Note that adding and removing constraints is recorded in the transaction log."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e60e9eba-5534-4d6a-8528-5eced13eb36b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "DESCRIBE HISTORY heart_rate_silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da378ac2-d2da-453a-a3e7-03c7f2558959",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define a Streaming Read and Transformation\n",
    "Using the cell below we will create a streaming read that includes:\n",
    "1. A filter for the topic **`bpm`**\n",
    "2. Logic to flatten the JSON payload and cast data to the appropriate schema\n",
    "3. A **`bpm_check`** column to flag negative records\n",
    "4. A duplicate check on **`device_id`** and **`time`** with a 30 second watermark on **`time`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdd85065-a0da-4f57-94e3-bd2ce9524c69",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "streaming_df = (spark.readStream\n",
    "                     .table(\"bronze\")\n",
    "                     .filter(\"topic = 'bpm'\")\n",
    "                     .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                     .select(\"v.*\", F.when(F.col(\"v.heartrate\") <= 0, \"Negative BPM\")\n",
    "                                     .otherwise(\"OK\")\n",
    "                                     .alias(\"bpm_check\"))\n",
    "                     .withWatermark(\"time\", \"30 seconds\")\n",
    "                     .dropDuplicates([\"device_id\", \"time\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "23d9574a-4533-4b8c-bf5c-961f84c6b4e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Upsert Query\n",
    "Below, the upsert class used in the previous notebooks is provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c7c1651-1f9b-4103-919c-ff4af0f1fe24",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upsert:\n",
    "    def __init__(self, sql_query, update_temp=\"stream_updates\"):\n",
    "        self.sql_query = sql_query\n",
    "        self.update_temp = update_temp \n",
    "        \n",
    "    def upsert_to_delta(self, micro_batch_df, batch):\n",
    "        micro_batch_df.createOrReplaceTempView(self.update_temp)\n",
    "        micro_batch_df._jdf.sparkSession().sql(self.sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3aa05d74-59e5-44a1-9a69-3701c36dcc30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use the cell below to define the upsert query to instantiate our class. \n",
    "\n",
    "Alternatetively, <a href=\"https://docs.databricks.com/delta/delta-update.html#upsert-into-a-table-using-merge&language-python\" target=\"_blank\">consult the documentation</a> and try implementing this using the **`DeltaTable`** Python class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "548bdd7b-ad0e-4b79-9f40-46c2cff67bbf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "sql_query = \"\"\"<FILL-IN>\"\"\"\n",
    " \n",
    "streaming_merge=Upsert(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d03de72a-9901-49d0-bccc-a6dab54c32e8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Apply Upsert and Write\n",
    "Now execute a write with trigger-available-now logic to process all existing data from the bronze table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "94cbe899-cbf9-4ed9-ad42-cc0a9b6ea753",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def process_silver_heartrate():\n",
    "    query = (streaming_df.writeStream\n",
    "                         .foreachBatch(streaming_merge.upsert_to_delta)\n",
    "                         .outputMode(\"update\")\n",
    "                         .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/recordings\")\n",
    "                         .trigger(availableNow=True)\n",
    "                         .start())\n",
    "    query.awaitTermination()\n",
    "    \n",
    "process_silver_heartrate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e89e9d9-d46b-4121-a2c0-0ae25f99a6f3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We should see the same number of total records in our silver table as the deduplicated count from the lesson 2.5, and a small percentage of these will correctly be flagged with \"Negative BPM\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d23ec379-f0bb-4d89-8645-5528db3080a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_total = spark.read.table(\"heart_rate_silver\").count()\n",
    "\n",
    "print(f\"Lesson #5: {731987:,}\")\n",
    "print(f\"New Total: {new_total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17dc892c-a3f2-4b73-aee3-6c8dc842504e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT COUNT(*)\n",
    "FROM heart_rate_silver\n",
    "WHERE bpm_check = \"Negative BPM\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec9eecdf-eb83-4171-a318-68906b12d50a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now land a new batch of data and propagate changes through bronze into the silver table.\n",
    "\n",
    "<img src=\"https://files.training.databricks.com/images/icon_note_32.png\"> The following two methods were recreated for us from previous lessons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90346b7e-6733-4e3a-95b0-6bc082e4a8e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.daily_stream.load() # Load a day's worth of data\n",
    "DA.process_bronze()    # Execute 1 iteration of the daily to bronze stream\n",
    "\n",
    "process_silver_heartrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1738e092-bd21-495e-ad2b-b3e9e39efd46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "end_total = spark.read.table(\"heart_rate_silver\").count()\n",
    "\n",
    "print(f\"Lesson #5:   {731987:,}\")\n",
    "print(f\"New Total:   {new_total:,}\")\n",
    "print(f\"End Total: {end_total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58ee8aeb-977e-40b0-8c05-1947de462051",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0363cdac-916a-404d-b73d-09716955cc6d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d1e450fb-68d5-4bb7-81c9-92986e39d264",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ADE 3.3 - Promoting to Silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}