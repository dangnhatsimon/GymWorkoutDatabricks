{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b08e111-b470-4383-85ba-fef9073beb81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n",
    "  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b93a6ffb-4952-43d5-9b46-993936149949",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Streaming Deduplication\n",
    "\n",
    "In this notebook, you'll learn how to eliminate duplicate records while working with Structured Streaming and Delta Lake. While Spark Structured Streaming provides exactly-once processing guarantees, many source systems will introduce duplicate records, which must be removed in order for joins and updates to produce logically correct results in downstream queries.\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this lesson, you should be able to:\n",
    "- Apply **`dropDuplicates`** to streaming data\n",
    "- Use watermarking to manage state information\n",
    "- Write an insert-only merge to prevent inserting duplicate records into a Delta table\n",
    "- Use **`foreachBatch`** to perform a streaming upsert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68f8aec8-75ef-4d58-a58f-86cbe5ada7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Setup\n",
    "Declare a database and set all path variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c3d588e4-6425-415d-8fb8-17f3ee1571c0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ../Includes/Classroom-Setup-4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69b44b9b-d5b2-4e5a-a49e-f4801b2da7b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Identify Duplicate Records\n",
    "\n",
    "Because Kafka provides at-least-once guarantees on data delivery, all Kafka consumers should be prepared to handle duplicate records.\n",
    "\n",
    "The de-duplication methods shown here can also be applied when necessary in other parts of your Delta Lake applications.\n",
    "\n",
    "Let's start by identifying the number of duplicate records in our **`bpm`** topic of the bronze table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ba68820-1faa-4ccd-a217-30eae950991e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "total = (spark.read\n",
    "              .table(\"bronze\")\n",
    "              .filter(\"topic = 'bpm'\")\n",
    "              .count())\n",
    "\n",
    "print(f\"Total: {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33c451d9-30a7-4a0c-aec2-d9a53eb6c4fa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "old_total = (spark.read\n",
    "                  .table(\"bronze\")\n",
    "                  .filter(\"topic = 'bpm'\")\n",
    "                  .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                  .select(\"v.*\")\n",
    "                  .dropDuplicates([\"device_id\", \"time\"])\n",
    "                  .count())\n",
    "\n",
    "print(f\"Old Total: {old_total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd114f7d-96e7-46b8-9593-ab9509804ae0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "It appears that around 10-20% of our records are duplicates. Note that here we're choosing to apply deduplication at the silver rather than the bronze level. While we are storing some duplicate records, our bronze table retains a history of the true state of our streaming source, presenting all records as they arrived (with some additional metadata recorded). This allows us to recreate any state of our downstream system, if necessary, and prevents potential data loss due to overly aggressive quality enforcement at the initial ingestion as well as minimizing latencies for data ingestion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f2cd5578-66a4-4be7-b227-ff495f8e7195",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define a Streaming Read on the Bronze BPM Records\n",
    "\n",
    "Here we'll bring back in our final logic from our last notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3b527ea5-346c-4c9b-b1ce-5286ad5e4d71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "bpm_df = (spark.readStream\n",
    "               .table(\"bronze\")\n",
    "               .filter(\"topic = 'bpm'\")\n",
    "               .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "               .select(\"v.*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f605d008-8149-44eb-b890-58ed56602910",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "When dealing with streaming deduplication, there is a level of complexity compared to static data.\n",
    "\n",
    "As each micro-batch is processed, we need to ensure:\n",
    "- No duplicate records exist in the microbatch\n",
    "- Records to be inserted are not already in the target table\n",
    "\n",
    "Spark Structured Streaming can track state information for the unique keys to ensure that duplicate records do not exist within or between microbatches. Over time, this state information will scale to represent all history. Applying a watermark of appropriate duration allows us to only track state information for a window of time in which we reasonably expect records could be delayed. Here, we'll define that watermark as 30 seconds.\n",
    "\n",
    "The cell below updates our previous query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "02c78a3c-a26b-440e-8add-2d60a10420aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "json_schema = \"device_id LONG, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "deduped_df = (spark.readStream\n",
    "                   .table(\"bronze\")\n",
    "                   .filter(\"topic = 'bpm'\")\n",
    "                   .select(F.from_json(F.col(\"value\").cast(\"string\"), json_schema).alias(\"v\"))\n",
    "                   .select(\"v.*\")\n",
    "                   .withWatermark(\"time\", \"30 seconds\")\n",
    "                   .dropDuplicates([\"device_id\", \"time\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f3e6b0b4-a0a1-4f97-80e6-f0b175415af7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Insert Only Merge\n",
    "Delta Lake has optimized functionality for insert-only merges. This operation is ideal for de-duplication: define logic to match on unique keys, and only insert those records for keys that don't already exist.\n",
    "\n",
    "Note that in this application, we proceed in this fashion because we know two records with the same matching keys represent the same information. If the later arriving records indicated a necessary change to an existing record, we would need to change our logic to include a **`WHEN MATCHED`** clause.\n",
    "\n",
    "A merge into query is defined in SQL below against a view titled **`stream_updates`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a57dd682-149d-486a-b2f5-07dae7536c10",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sql_query = \"\"\"\n",
    "  MERGE INTO heart_rate_silver a\n",
    "  USING stream_updates b\n",
    "  ON a.device_id=b.device_id AND a.time=b.time\n",
    "  WHEN NOT MATCHED THEN INSERT *\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2a4712c-12c1-437f-b48c-c3c2ff380faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Defining a Microbatch Function for **`foreachBatch`**\n",
    "\n",
    "The Spark Structured Streaming **`foreachBatch`** method allows users to define custom logic when writing.\n",
    "\n",
    "The logic applied during **`foreachBatch`** addresses the present microbatch as if it were a batch (rather than streaming) data.\n",
    "\n",
    "The class defined in the following cell defines simple logic that will allow us to register any SQL **`MERGE INTO`** query for use in a Structured Streaming write."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2ad8a5c8-b10a-42aa-b508-ebfca2131657",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upsert:\n",
    "    def __init__(self, sql_query, update_temp=\"stream_updates\"):\n",
    "        self.sql_query = sql_query\n",
    "        self.update_temp = update_temp \n",
    "        \n",
    "    def upsert_to_delta(self, microBatchDF, batch):\n",
    "        microBatchDF.createOrReplaceTempView(self.update_temp)\n",
    "        microBatchDF._jdf.sparkSession().sql(self.sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e794e317-0c92-4e91-948c-a13a8833d4ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Because we're using SQL to write to our Delta table, we'll need to make sure this table exists before we begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74a9d3a8-1ae0-4f4a-8e66-48960702d934",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "CREATE TABLE IF NOT EXISTS heart_rate_silver \n",
    "(device_id LONG, time TIMESTAMP, heartrate DOUBLE)\n",
    "USING DELTA\n",
    "LOCATION '${da.paths.user_db}/heart_rate_silver'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9acec8bb-5f2a-4738-8576-f8fd61008c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Now pass the previously defined **`sql_query`** to the **`Upsert`** class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2bd7294e-7f47-42b6-8834-5221c263b7fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_merge = Upsert(sql_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7daa7ff1-a2a7-4800-bc87-8f4a4f8d9c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "And then use this class in our **`foreachBatch`** logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "947e980f-9d76-4b59-99c6-e32e989caafe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = (deduped_df.writeStream\n",
    "                   .foreachBatch(streaming_merge.upsert_to_delta)\n",
    "                   .outputMode(\"update\")\n",
    "                   .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/recordings\")\n",
    "                   .trigger(availableNow=True)\n",
    "                   .start())\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3dfad58d-3599-49d1-8bcc-b1e9b1047386",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We can see that our number of unique entries that have been processed to the **`heart_rate_silver`** table matches our batch de-duplication query from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00ea21f5-0bad-419b-8db5-5f4c0b642f1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "new_total = spark.read.table(\"heart_rate_silver\").count()\n",
    "\n",
    "print(f\"Old Total: {old_total:,}\")\n",
    "print(f\"New Total: {new_total:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de1cfc0f-0305-4174-a74c-4fffac922c37",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Run the following cell to delete the tables and files associated with this lesson."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ba229a7-b720-4959-bed8-48a35a3f8581",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DA.cleanup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eccd7557-1776-4fa0-81d4-62184246b2e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "&copy; 2022 Databricks, Inc. All rights reserved.<br/>\n",
    "Apache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"https://www.apache.org/\">Apache Software Foundation</a>.<br/>\n",
    "<br/>\n",
    "<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"https://help.databricks.com/\">Support</a>"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ADE 3.1 - Streaming Deduplication",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}